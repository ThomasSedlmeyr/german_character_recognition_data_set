{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The following Jupyter-Notebook shows how to train a simple PyTorch classifier on the **German Character Recognition Dataset**. The trained network achieves an ACC- and MCC-value of more than 0.99.   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we define some global variables which are used for the whole training process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Change the paths accordingly\n",
    "path_train_csv = \"/kaggle/input/german-character-recognition-dataset/train.csv\"\n",
    "path_test_csv = \"/kaggle/input/german-character-recognition-dataset/test.csv\"\n",
    "# First we have to select the classes on which we would like to train on\n",
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "# All available classes which are contained in the dataset\n",
    "#classes = ['!','$','&','(',')','+','0','1','2','3','4','5','6','7','8','9','<','>','?','A','B','C','D','E','F','G','H',\n",
    "#           'I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i',\n",
    "#           'j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','~','ß','α','β','π','φ','€','∑','√','∞',\n",
    "#           '∫']\n",
    "dict_classes_to_numbers = dict(zip(classes, range(len(classes))))\n",
    "dict_numbers_to_classes = dict(zip(range(len(classes)), classes))\n",
    "num_classes = len(classes)\n",
    "print(\"Num classes: \" + str(num_classes))\n",
    "num_val_samples_per_class = 250\n",
    "# Standard DL-parameters\n",
    "batch_size_train = 128\n",
    "batch_size_val = 256\n",
    "num_workers = 2\n",
    "lr = 0.001\n",
    "hparams = {\"num_epochs\": 100, \"early_stopping_patience\": 5, \"early_stopping_threshold\": 0.001}\n",
    "# For getting reproducible results\n",
    "seed = 0\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:27:48.637111849Z",
     "start_time": "2023-09-12T19:27:48.064353464Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:39:20.699097Z",
     "iopub.execute_input": "2023-09-14T19:39:20.699613Z",
     "iopub.status.idle": "2023-09-14T19:39:24.793524Z",
     "shell.execute_reply.started": "2023-09-14T19:39:20.699581Z",
     "shell.execute_reply": "2023-09-14T19:39:24.792457Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Num classes: 10\n",
     "output_type": "stream"
    },
    {
     "execution_count": 1,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<torch._C.Generator at 0x7a8bc0a42310>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define some helper functions for the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0.001, model_weights=None):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_validation_acc = 0\n",
    "        self.best_model_weights = model_weights\n",
    "\n",
    "    def early_stop(self, validation_acc, model_weights):\n",
    "       # If the model improved we store the best weights\n",
    "        if validation_acc > self.best_validation_acc:\n",
    "            self.best_model_weights = model_weights\n",
    "        \n",
    "        # If the model improved more than the threshold  \n",
    "        if validation_acc > self.best_validation_acc + self.min_delta:\n",
    "            self.counter = 0\n",
    "            self.best_validation_acc = validation_acc\n",
    "        else:          \n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            self.counter += 1    \n",
    "        return False\n",
    "\n",
    "\n",
    "class EpochInformation:\n",
    "    def __init__(self, model, device, num_classes, dataset_sizes):\n",
    "        self.mcc_metric = torchmetrics.MatthewsCorrCoef(task='multiclass', num_classes=num_classes).to(device)\n",
    "        self.auc_metric = torchmetrics.AUROC(task='multiclass', num_classes=num_classes).to(device)\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "        self.running_loss = 0.0\n",
    "        self.running_outputs = None\n",
    "        self.running_labels = None\n",
    "        self.model = model\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        self.running_loss = 0.0\n",
    "        self.running_outputs = None\n",
    "        self.running_labels = None            \n",
    "        \n",
    "    def update_metrics_for_batch(self, outputs, loss, inputs, labels):\n",
    "        if self.running_outputs is None:\n",
    "            self.running_outputs = outputs\n",
    "        else:\n",
    "            self.running_outputs = torch.cat((self.running_outputs, outputs), dim=0)\n",
    "        \n",
    "        if self.running_labels is None:\n",
    "            self.running_labels = labels\n",
    "        else:\n",
    "            self.running_labels = torch.cat((self.running_labels, labels), dim=0)\n",
    "\n",
    "        #update the loss\n",
    "        self.running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    def calculate_metrics(self, phase):\n",
    "        loss = self.running_loss / self.dataset_sizes[phase]\n",
    "                \n",
    "        _, predictions = torch.max(self.running_outputs, 1)\n",
    "        comparison = predictions == self.running_labels\n",
    "        corrects = torch.sum(comparison)\n",
    "        \n",
    "        acc = corrects.double() / self.dataset_sizes[phase]\n",
    "        mcc = self.mcc_metric(predictions, self.running_labels)\n",
    "        auc = self.auc_metric(self.running_outputs, self.running_labels)\n",
    "        \n",
    "        # The gradient norm can only be calculated during training\n",
    "        # Also we calculate the weight-norm only once in each training epoch\n",
    "        if self.model.training:\n",
    "            grads = [param.grad.detach().flatten() for param in self.model.parameters() if param.grad is not None]\n",
    "            l2_norm_grads = torch.linalg.vector_norm(torch.cat(grads))\n",
    "            weights = [param.detach().flatten() for param in self.model.parameters()]\n",
    "            l2_norm_weights = torch.linalg.vector_norm(torch.cat(weights)) \n",
    "           \n",
    "        result_dict = {\n",
    "            \"loss\" : loss,\n",
    "            \"acc\"  : acc.item(),\n",
    "            \"mcc\"  : mcc.item(),\n",
    "            \"auc\"  : auc.item()\n",
    "        }\n",
    "        if self.model.training:\n",
    "            result_dict[\"l2_grad\"] = l2_norm_grads.item()\n",
    "            result_dict[\"l2_weights\"] = l2_norm_weights.item()\n",
    "            \n",
    "        return result_dict "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:27:48.668337508Z",
     "start_time": "2023-09-12T19:27:48.101865843Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:39:24.795872Z",
     "iopub.execute_input": "2023-09-14T19:39:24.796407Z",
     "iopub.status.idle": "2023-09-14T19:39:36.102294Z",
     "shell.execute_reply.started": "2023-09-14T19:39:24.796361Z",
     "shell.execute_reply": "2023-09-14T19:39:36.101315Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define a simple CNN-model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorchClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PyTorchClassifier, self).__init__()\n",
    "        self.size_fc1 = 1024\n",
    "        self.conv1 = nn.Conv2d(1, 32, 6, padding=\"same\")\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, padding=\"same\")\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, padding=\"same\")\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 2, padding=\"same\")\n",
    "        self.conv5 = nn.Conv2d(256, 256, 2)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(self.size_fc1, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(F.relu(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:27:48.672750935Z",
     "start_time": "2023-09-12T19:27:48.151292007Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:39:36.103860Z",
     "iopub.execute_input": "2023-09-14T19:39:36.104209Z",
     "iopub.status.idle": "2023-09-14T19:39:36.118711Z",
     "shell.execute_reply.started": "2023-09-14T19:39:36.104175Z",
     "shell.execute_reply": "2023-09-14T19:39:36.117183Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code for generating the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class GermanCharacterRecognitionDS(Dataset):\n",
    "    def __init__(self, path_csv, transform=None, target_transform=None, classes=[],\n",
    "                 num_channels=1):\n",
    "        self.path_csv = path_csv\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_lines = self.read_lines_csv(classes)\n",
    "        self.n = len(self.data_lines)\n",
    "        self.classes = classes\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, image = self.parse_one_line(idx)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)            \n",
    "        # We have to convert the label to an integer value\n",
    "        label = dict_classes_to_numbers[label]\n",
    "        return image, label\n",
    "\n",
    "    def read_lines_csv(self, classes):\n",
    "        training_data_file = open(self.path_csv, 'r')\n",
    "        data_lines = training_data_file.readlines()\n",
    "        training_data_file.close()\n",
    "        data_lines = [line for line in data_lines if line[0] in classes]\n",
    "        return data_lines\n",
    "\n",
    "    def parse_one_line(self, index):\n",
    "        line = self.data_lines[index].split(',')\n",
    "        image_np = np.asarray(line[1:1601], dtype=np.float32)\n",
    "        image_np = image_np.reshape(40, 40, 1)\n",
    "        if self.num_channels != 1:\n",
    "            image_np = np.repeat(image_np, self.num_channels, axis=2)\n",
    "        return line[0], image_np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:27:48.675320389Z",
     "start_time": "2023-09-12T19:27:48.242957850Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:39:36.121768Z",
     "iopub.execute_input": "2023-09-14T19:39:36.122295Z",
     "iopub.status.idle": "2023-09-14T19:39:36.141606Z",
     "shell.execute_reply.started": "2023-09-14T19:39:36.122261Z",
     "shell.execute_reply": "2023-09-14T19:39:36.140625Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can define the train loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "def train_model(data_loaders, model, loss_func, optimizer, device):\n",
    "    print(\"training started\")\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    information = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "    early_stopper = EarlyStopper(patience=hparams[\"early_stopping_patience\"],\n",
    "                             min_delta=hparams[\"early_stopping_threshold\"],\n",
    "                             model_weights=copy.deepcopy(model.state_dict()))\n",
    "    strop_training = False\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        if strop_training == True:\n",
    "            break\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()  \n",
    "            information.reset_metrics()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                print(\"training...\")\n",
    "            else:\n",
    "                print(\"validating...\")                \n",
    "            data_loader = tqdm(data_loaders[phase])\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                information.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "            result_dict = information.calculate_metrics(phase)\n",
    "            # prints the all metrics of the training and validation phase\n",
    "            print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if early_stopper.early_stop(result_dict[\"mcc\"], copy.deepcopy(model.state_dict())):\n",
    "                    print('early stopping')\n",
    "                    strop_training = True\n",
    "    # load best model\n",
    "    model.load_state_dict(early_stopper.best_model_weights) \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:27:48.675522407Z",
     "start_time": "2023-09-12T19:27:48.243585454Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:39:36.143267Z",
     "iopub.execute_input": "2023-09-14T19:39:36.143617Z",
     "iopub.status.idle": "2023-09-14T19:39:36.162709Z",
     "shell.execute_reply.started": "2023-09-14T19:39:36.143586Z",
     "shell.execute_reply": "2023-09-14T19:39:36.161597Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For loading the data we need some helper methods. As stated in the description of the dataset, the representation of each class of the train data set is the same for each class. We also want to make sure that the validation data has the same distribution as the test data, so we need a function which takes a certain amount of samples from each class of the train data set and puts them into the validation data set. To optimize the run time, we save the indices of the train and validation data set in a numpy array. This approach eliminates the need to regenerate the data split each time, thereby significantly reducing processing time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_train_and_val_set(train_data_set, num_samples_validation_data=250):\n",
    "    # Define the ratio for train and validation data\n",
    "    print(\"Splitting train- and val-data ...\")\n",
    "    val_count = dict(zip(classes, len(classes) * [0]))\n",
    "    val_indices = []\n",
    "    train_indices = []\n",
    "    for i in range(len(train_data_set)):\n",
    "        _, label = train_data_set[i]\n",
    "        label_string = dict_numbers_to_classes[label]\n",
    "        if val_count[label_string] < num_samples_validation_data:\n",
    "            val_count[label_string] += 1\n",
    "            val_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "\n",
    "    np.save(\"val_indices.npy\", np.asarray(val_indices))\n",
    "    np.save(\"train_indices.npy\", np.asarray(train_indices))\n",
    "    train_set, val_set = split_train_set(train_data_set, train_indices, val_indices)\n",
    "    print(\"Splitting done\")\n",
    "    return train_set, val_set\n",
    "\n",
    "def split_train_set(train_data_set, train_indices, val_indices):\n",
    "    train_set = Subset(train_data_set, train_indices)\n",
    "    val_set = Subset(train_data_set, val_indices)\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_class_counts_of_data_loader(data_loader):\n",
    "    labels_count_dict = dict(zip(classes, len(classes) * [0]))\n",
    "    for _, labels in data_loader:\n",
    "        string_labels = [dict_numbers_to_classes[number] for number in labels.tolist()]\n",
    "        for label in string_labels:\n",
    "            labels_count_dict[label] += 1\n",
    "    return labels_count_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:27:48.675634494Z",
     "start_time": "2023-09-12T19:27:48.286722525Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:39:36.164296Z",
     "iopub.execute_input": "2023-09-14T19:39:36.164643Z",
     "iopub.status.idle": "2023-09-14T19:39:36.181980Z",
     "shell.execute_reply.started": "2023-09-14T19:39:36.164612Z",
     "shell.execute_reply": "2023-09-14T19:39:36.181118Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can proceed to construct all the necessary data loaders."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from random import random\n",
    "\n",
    "# We normalize with the men and std of the train set\n",
    "standard_transforms = [transforms.ToTensor(),transforms.Normalize(35.37502147246886, 75.87412766890324)]\n",
    "test_set = GermanCharacterRecognitionDS(path_test_csv, transform=transforms.Compose(standard_transforms), classes=classes,\n",
    "                                        num_channels=1)\n",
    "train_set = GermanCharacterRecognitionDS(path_train_csv, transform=None, classes=classes,\n",
    "                                         num_channels=1)\n",
    "num_train = len(train_set)\n",
    "num_test = len(test_set)\n",
    "print(\"number Train set: \" + str(num_train))\n",
    "print(\"number Test set: \" + str(num_test))\n",
    "print(\"number whole ds: \" + str(num_test + num_train))\n",
    "# TODO comment the following line after the first run\n",
    "train_set, val_set = get_train_and_val_set(train_set, num_val_samples_per_class)\n",
    "# TODO uncomment this line if you want to use the precalculated indices which speeds up the run time\n",
    "# train_set, val_set = split_train_set(train_set, np.load(\"train_indices.npy\"), np.load(\"val_indices.npy\"))\n",
    "\n",
    "train_transforms = standard_transforms + [transforms.RandomRotation(30), transforms.RandomGrayscale(p=0.1), \n",
    "                                          transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))]\n",
    "train_set.dataset.transform = transforms.Compose(train_transforms)\n",
    "val_set.dataset.transform = transforms.Compose(standard_transforms)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True, num_workers=num_workers,                                                   generator=g)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size_val, shuffle=False, num_workers=num_workers,\n",
    "                                         generator=g)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_val, shuffle=False, num_workers=num_workers,\n",
    "                                          generator=g)\n",
    "\n",
    "class_counts_train = get_class_counts_of_data_loader(train_loader)\n",
    "class_counts_val = get_class_counts_of_data_loader(val_loader)\n",
    "class_counts_test = get_class_counts_of_data_loader(test_loader)\n",
    "\n",
    "print(\"train_loader: \" + str(class_counts_train))\n",
    "print(\"val_loader: \" + str(class_counts_val))\n",
    "print(\"test_loader: \" + str(class_counts_test))\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": len(train_loader.dataset), \"val\": len(val_loader.dataset), \"test\": len(test_loader.dataset)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:28:19.732966996Z",
     "start_time": "2023-09-12T19:27:48.300056250Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:39:36.184483Z",
     "iopub.execute_input": "2023-09-14T19:39:36.185092Z",
     "iopub.status.idle": "2023-09-14T19:40:30.156043Z",
     "shell.execute_reply.started": "2023-09-14T19:39:36.185060Z",
     "shell.execute_reply": "2023-09-14T19:40:30.154820Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "number Train set: 44251\nnumber Test set: 5000\nnumber whole ds: 49251\nSplitting train- and val-data ...\nSplitting done\ntrain_loader: {'0': 4541, '1': 4203, '2': 4168, '3': 4120, '4': 4019, '5': 3966, '6': 4235, '7': 4161, '8': 4213, '9': 4125}\nval_loader: {'0': 250, '1': 250, '2': 250, '3': 250, '4': 250, '5': 250, '6': 250, '7': 250, '8': 250, '9': 250}\ntest_loader: {'0': 500, '1': 500, '2': 500, '3': 500, '4': 500, '5': 500, '6': 500, '7': 500, '8': 500, '9': 500}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also calculate the class weights in order to use a weighted loss function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class_weights = []\n",
    "number_train_values = len(train_loader.dataset)\n",
    "for class_label in classes:\n",
    "    weight = float(number_train_values) / class_counts_train[class_label]\n",
    "    class_weights.append(weight)\n",
    "class_weights = torch.tensor(class_weights)\n",
    "sum_class_weights = torch.sum(class_weights)\n",
    "class_weights = class_weights / sum_class_weights\n",
    "print(\"class weights: \", str(dict(zip(classes, class_weights.tolist()))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:28:19.746579889Z",
     "start_time": "2023-09-12T19:28:19.743004011Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:40:30.157510Z",
     "iopub.execute_input": "2023-09-14T19:40:30.157867Z",
     "iopub.status.idle": "2023-09-14T19:40:30.167318Z",
     "shell.execute_reply.started": "2023-09-14T19:40:30.157839Z",
     "shell.execute_reply": "2023-09-14T19:40:30.166123Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "class weights:  {'0': 0.09183375537395477, '1': 0.09921891242265701, '2': 0.10005208849906921, '3': 0.10121773928403854, '4': 0.10376140475273132, '5': 0.10514803230762482, '6': 0.09846921265125275, '7': 0.10022040456533432, '8': 0.09898340702056885, '9': 0.101095050573349}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#from torchsummary import summary\n",
    "model = PyTorchClassifier(len(classes))\n",
    "# print the model\n",
    "#summary(model, (1, 40, 40))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Put the tensors on the GPU\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:28:19.907848428Z",
     "start_time": "2023-09-12T19:28:19.747857170Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:40:30.168991Z",
     "iopub.execute_input": "2023-09-14T19:40:30.169523Z",
     "iopub.status.idle": "2023-09-14T19:40:35.541431Z",
     "shell.execute_reply.started": "2023-09-14T19:40:30.169491Z",
     "shell.execute_reply": "2023-09-14T19:40:35.540376Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "cuda:0\n",
     "output_type": "stream"
    },
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "PyTorchClassifier(\n  (conv1): Conv2d(1, 32, kernel_size=(6, 6), stride=(1, 1), padding=same)\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1), padding=same)\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=same)\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv4): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1), padding=same)\n  (conv5): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=256, out_features=10, bias=True)\n)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can start the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class_weights = class_weights.to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=lr)\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "model = train_model(data_loaders, model, loss_func, optimizer, device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-14T19:40:35.544816Z",
     "iopub.execute_input": "2023-09-14T19:40:35.545112Z",
     "iopub.status.idle": "2023-09-14T19:46:23.423058Z",
     "shell.execute_reply.started": "2023-09-14T19:40:35.545088Z",
     "shell.execute_reply": "2023-09-14T19:46:23.421909Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "training started\nEpoch 0/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "  0%|          | 0/10 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/Convolution.cpp:1003.)\n  return F.conv2d(input, weight, bias, self.stride,\n100%|██████████| 10/10 [00:05<00:00,  1.75it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 2.3035 acc: 0.1 mcc: 0 auc: 0.5338\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 17.02it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.3656 acc: 0.8802 mcc: 0.8671 auc: 0.9916 l2_grad: 1.3001 l2_weights: 23.8132\nEpoch 1/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.52it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0596 acc: 0.9828 mcc: 0.9809 auc: 0.9998\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:21<00:00, 15.38it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0808 acc: 0.9764 mcc: 0.9738 auc: 0.9994 l2_grad: 3.2679 l2_weights: 26.7038\nEpoch 2/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.41it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0816 acc: 0.9788 mcc: 0.9765 auc: 0.9996\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 17.04it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0589 acc: 0.9828 mcc: 0.9809 auc: 0.9996 l2_grad: 0.0117 l2_weights: 29.0084\nEpoch 3/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.69it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.062 acc: 0.9828 mcc: 0.9809 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:18<00:00, 18.05it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0487 acc: 0.9861 mcc: 0.9845 auc: 0.9997 l2_grad: 0.1906 l2_weights: 31.1929\nEpoch 4/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.41it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0519 acc: 0.9864 mcc: 0.9849 auc: 0.9998\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 16.98it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0429 acc: 0.9879 mcc: 0.9866 auc: 0.9998 l2_grad: 0.2167 l2_weights: 33.4229\nEpoch 5/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.63it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0558 acc: 0.9868 mcc: 0.9853 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 17.04it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0368 acc: 0.9897 mcc: 0.9885 auc: 0.9998 l2_grad: 0.0177 l2_weights: 35.6054\nEpoch 6/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.23it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0665 acc: 0.9836 mcc: 0.9818 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:18<00:00, 17.95it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.033 acc: 0.9905 mcc: 0.9895 auc: 0.9999 l2_grad: 0.0008 l2_weights: 37.6541\nEpoch 7/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.53it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0521 acc: 0.9884 mcc: 0.9871 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 16.95it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.031 acc: 0.9907 mcc: 0.9896 auc: 0.9999 l2_grad: 4.1595 l2_weights: 39.8253\nEpoch 8/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.50it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.1567 acc: 0.9608 mcc: 0.9567 auc: 0.9989\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:18<00:00, 17.75it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0306 acc: 0.9911 mcc: 0.9901 auc: 0.9999 l2_grad: 0.1798 l2_weights: 42.0927\nEpoch 9/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.70it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0581 acc: 0.9888 mcc: 0.9876 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 17.16it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0268 acc: 0.9916 mcc: 0.9907 auc: 0.9999 l2_grad: 0.0161 l2_weights: 44.2798\nEpoch 10/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.19it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0507 acc: 0.99 mcc: 0.9889 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 17.05it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0231 acc: 0.9932 mcc: 0.9924 auc: 0.9999 l2_grad: 0.1838 l2_weights: 46.071\nEpoch 11/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.66it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0579 acc: 0.9892 mcc: 0.988 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:18<00:00, 17.96it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0203 acc: 0.9939 mcc: 0.9932 auc: 1.0 l2_grad: 0.3486 l2_weights: 47.7097\nEpoch 12/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.59it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0512 acc: 0.988 mcc: 0.9867 auc: 0.9998\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 17.06it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0191 acc: 0.9943 mcc: 0.9937 auc: 1.0 l2_grad: 0.2576 l2_weights: 49.4082\nEpoch 13/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.37it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0626 acc: 0.9892 mcc: 0.988 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 16.95it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0211 acc: 0.9938 mcc: 0.9931 auc: 1.0 l2_grad: 0.7074 l2_weights: 51.4441\nEpoch 14/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.38it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0593 acc: 0.988 mcc: 0.9867 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:18<00:00, 18.12it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0177 acc: 0.9946 mcc: 0.994 auc: 1.0 l2_grad: 0.0669 l2_weights: 53.4257\nEpoch 15/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.82it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0687 acc: 0.9888 mcc: 0.9876 auc: 0.9997\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:19<00:00, 16.98it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.019 acc: 0.9946 mcc: 0.994 auc: 1.0 l2_grad: 6.7176 l2_weights: 55.1883\nEpoch 16/99\n----------\nvalidating...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:01<00:00,  8.58it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0858 acc: 0.9792 mcc: 0.9769 auc: 0.9996\nearly stopping\ntraining...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 327/327 [00:18<00:00, 17.90it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss: 0.0171 acc: 0.9948 mcc: 0.9943 auc: 1.0 l2_grad: 0.3753 l2_weights: 56.9699\nEpoch 17/99\n----------\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the training we evaluate the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "information_test = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "model.eval()\n",
    "for inputs, labels in data_loaders[\"test\"]:\n",
    "    inputs = inputs.to(device, non_blocking=True)\n",
    "    labels = labels.to(device, non_blocking=True)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "    information_test.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "result_dict = information_test.calculate_metrics(\"test\")\n",
    "print(\"Test metrics:\")\n",
    "print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T19:31:30.933956090Z",
     "start_time": "2023-09-12T19:31:27.034108281Z"
    },
    "execution": {
     "iopub.status.busy": "2023-09-14T19:46:23.425028Z",
     "iopub.execute_input": "2023-09-14T19:46:23.426009Z",
     "iopub.status.idle": "2023-09-14T19:46:26.708013Z",
     "shell.execute_reply.started": "2023-09-14T19:46:23.425977Z",
     "shell.execute_reply": "2023-09-14T19:46:26.706847Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "Test metrics:\nloss: 0.0509 acc: 0.9884 mcc: 0.9871 auc: 0.9998\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
