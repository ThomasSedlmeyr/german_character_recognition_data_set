{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a83349ffb7c61d",
   "metadata": {},
   "source": [
    "# Training a simple PyTorch classifier on the German Character Recognition Dataset\n",
    "The following Jupyter-Notebook shows how to train a simple PyTorch classifier on the [German Character Recognition Dataset](https://www.kaggle.com/datasets/thomassedlmeyr/german-character-recognition-dataset). The trained network achieves an ACC- and MCC-value of roughly 0.99.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf25bc096cd0613",
   "metadata": {},
   "source": [
    "First we define some global variables which are used for the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87a7cd9d506d565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T12:14:08.802216774Z",
     "start_time": "2023-09-20T12:14:08.756537635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f763da86830>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Change the paths accordingly\n",
    "path_train_csv = \"../train.csv\"\n",
    "path_test_csv = \"../test.csv\"\n",
    "# First we have to select the classes on which we would like to train on\n",
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "# All available classes which are contained in the dataset\n",
    "#classes = ['!','$','&','(',')','+','0','1','2','3','4','5','6','7','8','9','<','>','?','A','B','C','D','E','F','G','H',\n",
    "#           'I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i',\n",
    "#           'j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','~','ß','α','β','π','φ','€','∑','√','∞',\n",
    "#           '∫']\n",
    "dict_classes_to_numbers = dict(zip(classes, range(len(classes))))\n",
    "dict_numbers_to_classes = dict(zip(range(len(classes)), classes))\n",
    "num_classes = len(classes)\n",
    "print(\"Num classes: \" + str(num_classes))\n",
    "num_val_samples_per_class = 250\n",
    "# Standard DL-parameters\n",
    "batch_size_train = 128\n",
    "batch_size_val = 256\n",
    "num_workers = 2\n",
    "lr = 0.001\n",
    "hparams = {\"num_epochs\": 100, \"early_stopping_patience\": 5, \"early_stopping_threshold\": 0.001}\n",
    "# For getting reproducible results\n",
    "seed = 0\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6071488dbb00355",
   "metadata": {},
   "source": [
    "We define some helper functions for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d1b686273cb0a",
   "metadata": {},
   "source": [
    "Then we can define the train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25f6fa6aa8aa886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T12:14:09.114068781Z",
     "start_time": "2023-09-20T12:14:08.773860463Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "from train_utils import EpochInformation\n",
    "from train_utils import EarlyStopper\n",
    "\n",
    "def train_model(data_loaders, model, loss_func, optimizer, device):\n",
    "    print(\"training started\")\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    information = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "    early_stopper = EarlyStopper(patience=hparams[\"early_stopping_patience\"],\n",
    "                             min_delta=hparams[\"early_stopping_threshold\"],\n",
    "                             model_weights=copy.deepcopy(model.state_dict()))\n",
    "    strop_training = False\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        if strop_training == True:\n",
    "            break\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()  \n",
    "            information.reset_metrics()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                print(\"training...\")\n",
    "            else:\n",
    "                print(\"validating...\")                \n",
    "            data_loader = tqdm(data_loaders[phase])\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                information.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "            result_dict = information.calculate_metrics(phase)\n",
    "            # prints the all metrics of the training and validation phase\n",
    "            print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if early_stopper.early_stop(result_dict[\"mcc\"], copy.deepcopy(model.state_dict())):\n",
    "                    print('early stopping')\n",
    "                    strop_training = True\n",
    "    # load best model\n",
    "    model.load_state_dict(early_stopper.best_model_weights) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2d1ba",
   "metadata": {},
   "source": [
    "For loading the data we need some helper methods. As stated in the description of the dataset, the representation of each class of the train data set is the same for each class. We also want to make sure that the validation data has the same distribution as the test data, so we need a function which takes a certain amount of samples from each class of the train data set and puts them into the validation data set. To optimize the run time, we save the indices of the train and validation data set in a numpy array. This approach eliminates the need to regenerate the data split each time, thereby significantly reducing processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2530143e3e55b010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T12:14:28.153415979Z",
     "start_time": "2023-09-20T12:14:09.156885091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum whole ds: 49251\n",
      "Splitting train- and val-data ...\n",
      "Splitting done\n",
      "train_loader: {'0': 4541, '1': 4203, '2': 4168, '3': 4120, '4': 4019, '5': 3966, '6': 4235, '7': 4161, '8': 4213, '9': 4125}\n",
      "val_loader: {'0': 250, '1': 250, '2': 250, '3': 250, '4': 250, '5': 250, '6': 250, '7': 250, '8': 250, '9': 250}\n",
      "test_loader: {'0': 500, '1': 500, '2': 500, '3': 500, '4': 500, '5': 500, '6': 500, '7': 500, '8': 500, '9': 500}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "from train_utils import get_train_and_val_set, get_class_counts_of_data_loader\n",
    "from data_pytorch import GermanCharacterRecognitionDS\n",
    "\n",
    "# We normalize with the men and std of the train set\n",
    "standard_transforms = [transforms.ToTensor(),transforms.Normalize(35.37502147246886, 75.87412766890324)]\n",
    "test_set = GermanCharacterRecognitionDS(path_test_csv, dict_classes_to_numbers, transform=transforms.Compose(standard_transforms), classes=classes,\n",
    "                                        num_channels=1)\n",
    "train_set = GermanCharacterRecognitionDS(path_train_csv, dict_classes_to_numbers, transform=None, classes=classes,\n",
    "                                         num_channels=1)\n",
    "num_train = len(train_set)\n",
    "num_test = len(test_set)\n",
    "print(\"sum whole ds: \" + str(num_train + num_test))\n",
    "# TODO comment the following line after the first run\n",
    "train_set, val_set = get_train_and_val_set(train_set, classes, dict_numbers_to_classes)\n",
    "# TODO uncomment this line if you want to use the precalculated indnum_val_samples_per_classices which speeds up the run time\n",
    "#train_set, val_set = split_train_set_from_indices(train_set, np.load(\"train_indices.npy\"), np.load(\"val_indices.npy\"))\n",
    "\n",
    "train_transforms = standard_transforms + [transforms.RandomRotation(30), transforms.RandomGrayscale(p=0.1), \n",
    "                                          transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))]\n",
    "train_set.dataset.transform = transforms.Compose(train_transforms)\n",
    "val_set.dataset.transform = transforms.Compose(standard_transforms)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True, num_workers=num_workers,                                                   generator=g)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size_val, shuffle=False, num_workers=num_workers,\n",
    "                                         generator=g)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_val, shuffle=False, num_workers=num_workers,\n",
    "                                          generator=g)\n",
    "\n",
    "class_counts_train = get_class_counts_of_data_loader(train_loader, classes, dict_numbers_to_classes)\n",
    "class_counts_val = get_class_counts_of_data_loader(val_loader, classes, dict_numbers_to_classes)\n",
    "class_counts_test = get_class_counts_of_data_loader(test_loader, classes, dict_numbers_to_classes)\n",
    "\n",
    "print(\"train_loader: \" + str(class_counts_train))\n",
    "print(\"val_loader: \" + str(class_counts_val))\n",
    "print(\"test_loader: \" + str(class_counts_test))\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": len(train_loader.dataset), \"val\": len(val_loader.dataset), \"test\": len(test_loader.dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a555fe597dd9fe",
   "metadata": {},
   "source": [
    "We also calculate the class weights in order to use a weighted loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d702529aa63cfcce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T12:14:28.154039289Z",
     "start_time": "2023-09-20T12:14:28.153206292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights:  {'0': 0.09183375537395477, '1': 0.09921891242265701, '2': 0.10005208849906921, '3': 0.10121773928403854, '4': 0.10376140475273132, '5': 0.10514803230762482, '6': 0.09846921265125275, '7': 0.10022040456533432, '8': 0.09898340702056885, '9': 0.101095050573349}\n"
     ]
    }
   ],
   "source": [
    "class_weights = []\n",
    "number_train_values = len(train_loader.dataset)\n",
    "for class_label in classes:\n",
    "    weight = float(number_train_values) / class_counts_train[class_label]\n",
    "    class_weights.append(weight)\n",
    "class_weights = torch.tensor(class_weights)\n",
    "sum_class_weights = torch.sum(class_weights)\n",
    "class_weights = class_weights / sum_class_weights\n",
    "print(\"class weights: \", str(dict(zip(classes, class_weights.tolist()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f0a839f2f1ecc",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7674a4d86ea31a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T12:17:54.545846987Z",
     "start_time": "2023-09-20T12:17:52.490621111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 40, 40]           1,184\n",
      "         MaxPool2d-2           [-1, 32, 20, 20]               0\n",
      "            Conv2d-3           [-1, 64, 20, 20]          32,832\n",
      "         MaxPool2d-4           [-1, 64, 10, 10]               0\n",
      "            Conv2d-5          [-1, 128, 10, 10]         131,200\n",
      "         MaxPool2d-6            [-1, 128, 5, 5]               0\n",
      "            Conv2d-7            [-1, 256, 5, 5]         131,328\n",
      "            Conv2d-8            [-1, 256, 4, 4]         262,400\n",
      "         MaxPool2d-9            [-1, 256, 2, 2]               0\n",
      "          Dropout-10                 [-1, 1024]               0\n",
      "           Linear-11                  [-1, 256]         262,400\n",
      "          Dropout-12                  [-1, 256]               0\n",
      "           Linear-13                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 823,914\n",
      "Trainable params: 823,914\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.95\n",
      "Params size (MB): 3.14\n",
      "Estimated Total Size (MB): 4.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from network_pytorch import PyTorchClassifier\n",
    "\n",
    "model = PyTorchClassifier(len(classes))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "# print the model\n",
    "summary(model, (1, 40, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c0838451d1162",
   "metadata": {},
   "source": [
    "Now we can start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7db439103b25e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T12:21:03.354023445Z",
     "start_time": "2023-09-20T12:18:02.584947653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n",
      "Epoch 0/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.304 acc: 0.1 mcc: 0 auc: 0.4339\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:08<00:00, 39.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3854 acc: 0.8749 mcc: 0.861 auc: 0.9908 l2_grad: 1.0043 l2_weights: 23.6107\n",
      "Epoch 1/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0679 acc: 0.9832 mcc: 0.9813 auc: 0.9997\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:08<00:00, 36.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0774 acc: 0.9766 mcc: 0.974 auc: 0.9994 l2_grad: 4.4325 l2_weights: 25.8208\n",
      "Epoch 2/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 15.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4098 acc: 0.8956 mcc: 0.8891 auc: 0.9967\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:08<00:00, 37.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0675 acc: 0.9802 mcc: 0.978 auc: 0.9995 l2_grad: 0.0764 l2_weights: 28.1896\n",
      "Epoch 3/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0651 acc: 0.986 mcc: 0.9845 auc: 0.9996\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:11<00:00, 28.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0487 acc: 0.9866 mcc: 0.9851 auc: 0.9997 l2_grad: 0.3222 l2_weights: 29.9702\n",
      "Epoch 4/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0559 acc: 0.986 mcc: 0.9845 auc: 0.9997\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:12<00:00, 26.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0429 acc: 0.9878 mcc: 0.9865 auc: 0.9998 l2_grad: 0.193 l2_weights: 31.8139\n",
      "Epoch 5/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0571 acc: 0.9856 mcc: 0.984 auc: 0.9998\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:12<00:00, 26.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0387 acc: 0.9894 mcc: 0.9883 auc: 0.9998 l2_grad: 0.0021 l2_weights: 33.9051\n",
      "Epoch 6/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.057 acc: 0.9872 mcc: 0.9858 auc: 0.9998\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:11<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0322 acc: 0.991 mcc: 0.99 auc: 0.9998 l2_grad: 0.0222 l2_weights: 35.5847\n",
      "Epoch 7/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0494 acc: 0.9876 mcc: 0.9862 auc: 0.9998\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:11<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0276 acc: 0.9911 mcc: 0.9902 auc: 0.9999 l2_grad: 0.0418 l2_weights: 37.5314\n",
      "Epoch 8/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0546 acc: 0.9888 mcc: 0.9876 auc: 0.9997\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:10<00:00, 31.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0288 acc: 0.9917 mcc: 0.9908 auc: 0.9999 l2_grad: 0.0243 l2_weights: 39.6079\n",
      "Epoch 9/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0502 acc: 0.988 mcc: 0.9867 auc: 0.9998\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:10<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0251 acc: 0.9925 mcc: 0.9917 auc: 0.9999 l2_grad: 0.0147 l2_weights: 41.6525\n",
      "Epoch 10/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.054 acc: 0.9868 mcc: 0.9853 auc: 0.9997\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:12<00:00, 26.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0236 acc: 0.9926 mcc: 0.9918 auc: 0.9999 l2_grad: 0.0173 l2_weights: 43.5441\n",
      "Epoch 11/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.073 acc: 0.988 mcc: 0.9867 auc: 0.9996\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:13<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0243 acc: 0.9928 mcc: 0.992 auc: 0.9999 l2_grad: 0.0728 l2_weights: 45.747\n",
      "Epoch 12/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0574 acc: 0.988 mcc: 0.9867 auc: 0.9998\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:10<00:00, 30.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0224 acc: 0.9934 mcc: 0.9927 auc: 0.9999 l2_grad: 0.7372 l2_weights: 47.5216\n",
      "Epoch 13/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0693 acc: 0.9876 mcc: 0.9862 auc: 0.9997\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:13<00:00, 24.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0202 acc: 0.994 mcc: 0.9934 auc: 1.0 l2_grad: 0.0013 l2_weights: 49.2705\n",
      "Epoch 14/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0711 acc: 0.986 mcc: 0.9845 auc: 0.9996\n",
      "early stopping\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:11<00:00, 27.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0166 acc: 0.9949 mcc: 0.9943 auc: 1.0 l2_grad: 0.785 l2_weights: 50.8286\n",
      "Epoch 15/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = class_weights.to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=lr)\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "model = train_model(data_loaders, model, loss_func, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17441b37abf77f04",
   "metadata": {},
   "source": [
    "After the training we evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ec08f16abc393a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T12:22:54.644898188Z",
     "start_time": "2023-09-20T12:22:53.313854223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics:\n",
      "loss: 0.0494 acc: 0.9888 mcc: 0.9876 auc: 0.9997\n"
     ]
    }
   ],
   "source": [
    "information_test = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "model.eval()\n",
    "for inputs, labels in data_loaders[\"test\"]:\n",
    "    inputs = inputs.to(device, non_blocking=True)\n",
    "    labels = labels.to(device, non_blocking=True)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "    information_test.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "result_dict = information_test.calculate_metrics(\"test\")\n",
    "print(\"Test metrics:\")\n",
    "print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
