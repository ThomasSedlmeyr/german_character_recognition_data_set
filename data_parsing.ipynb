{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "First we define some global variables which are used for the whole training process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53a83349ffb7c61d"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Change the paths accordingly\n",
    "path_train_csv = \"train.csv\"\n",
    "path_test_csv = \"test.csv\"\n",
    "# First we have to select the classes on which we would like to train on\n",
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "# All available classes\n",
    "#classes = ['!','$','&','(',')','+','0','1','2','3','4','5','6','7','8','9','<','>','?','A','B','C','D','E','F','G','H',\n",
    "#           'I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i',\n",
    "#           'j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','~','ß','α','β','π','φ','€','∑','√','∞',\n",
    "#           '∫']\n",
    "num_classes = len(classes)\n",
    "num_val_samples_per_class = 250\n",
    "# Standard DL-parameters\n",
    "batch_size = 64\n",
    "num_workers = 12\n",
    "hparams = {\"num_epochs\": 100, \"early_stopping_patience\": 3, \"early_stopping_threshold\": 0.001}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T10:17:31.989329702Z",
     "start_time": "2023-09-10T10:17:31.563384371Z"
    }
   },
   "id": "f87a7cd9d506d565"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For training later on we need to generate a one_hot_encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c5fba019f66bf69"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "reshaped_classes = np.array(classes).reshape(-1, 1)\n",
    "onehot_encoder = onehot_encoder.fit(reshaped_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T10:17:32.006053585Z",
     "start_time": "2023-09-10T10:17:31.588109904Z"
    }
   },
   "id": "fc0a8c84612ac927"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define some helper functions for the training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6071488dbb00355"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0.001, model_weights=None):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_validation_acc = 0\n",
    "        self.best_model_weights = model_weights\n",
    "\n",
    "    def early_stop(self, validation_acc, model_weights):\n",
    "       # If the model improved we store the best weights\n",
    "        if validation_acc > self.best_validation_acc:\n",
    "            self.best_model_weights = model_weights\n",
    "        \n",
    "        # If the model improved more than the threshold  \n",
    "        if validation_acc > self.best_validation_acc + self.min_delta:\n",
    "            self.counter = 0\n",
    "            self.best_validation_acc = validation_acc\n",
    "        else:          \n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            self.counter += 1    \n",
    "        return False\n",
    "\n",
    "\n",
    "class EpochInformation:\n",
    "    def __init__(self, model, device, num_classes, dataset_sizes):\n",
    "        self.mcc_metric = torchmetrics.MatthewsCorrCoef(task='multiclass', num_classes=num_classes).to(device)\n",
    "        self.auc_metric = torchmetrics.AUROC(task='multiclass', num_classes=num_classes).to(device)\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "        self.running_loss = 0.0\n",
    "        self.running_outputs = None\n",
    "        self.running_labels = None\n",
    "        self.model = model\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        self.running_loss = 0.0\n",
    "        self.running_outputs = None\n",
    "        self.running_labels = None            \n",
    "        \n",
    "    def update_metrics_for_batch(self, outputs, loss, inputs, labels):\n",
    "        if self.running_outputs is None:\n",
    "            self.running_outputs = outputs\n",
    "        else:\n",
    "            self.running_outputs = torch.cat((self.running_outputs, outputs), dim=0)\n",
    "        \n",
    "        if self.running_labels is None:\n",
    "            self.running_labels = labels\n",
    "        else:\n",
    "            self.running_labels = torch.cat((self.running_labels, labels), dim=0)\n",
    "\n",
    "        #update the loss\n",
    "        self.running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    def calculate_metrics(self, phase):\n",
    "        loss = self.running_loss / self.dataset_sizes[phase]\n",
    "                \n",
    "        _, predictions = torch.max(self.running_outputs, 1)\n",
    "        _, target_indices = torch.max(self.running_labels, 1)\n",
    "        comparison = predictions == target_indices\n",
    "        corrects = torch.sum(comparison)\n",
    "        \n",
    "        acc = corrects.double() / self.dataset_sizes[phase]\n",
    "        mcc = self.mcc_metric(predictions, target_indices)\n",
    "        auc = self.auc_metric(self.running_outputs, target_indices)\n",
    "        \n",
    "        # The gradient norm can only be calculated during training\n",
    "        # Also we calculate the weight-norm only once in each training epoch\n",
    "        if self.model.training:\n",
    "            grads = [param.grad.detach().flatten() for param in self.model.parameters() if param.grad is not None]\n",
    "            l2_norm_grads = torch.linalg.vector_norm(torch.cat(grads))\n",
    "            weights = [param.detach().flatten() for param in self.model.parameters()]\n",
    "            l2_norm_weights = torch.linalg.vector_norm(torch.cat(weights)) \n",
    "           \n",
    "        result_dict = {\n",
    "            \"loss\" : loss,\n",
    "            \"acc\"  : acc.item(),\n",
    "            \"mcc\"  : mcc.item(),\n",
    "            \"auc\"  : auc.item()\n",
    "        }\n",
    "        if self.model.training:\n",
    "            result_dict[\"l2_grad\"] = l2_norm_grads.item()\n",
    "            result_dict[\"l2_weights\"] = l2_norm_weights.item()\n",
    "            \n",
    "        return result_dict "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T10:17:32.021864824Z",
     "start_time": "2023-09-10T10:17:31.598022529Z"
    }
   },
   "id": "5af3981cde2e146c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define a simple CNN-model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28ec067a47cc96dd"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorchClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PyTorchClassifier, self).__init__()\n",
    "        self.size_fc1 = 256\n",
    "        self.conv1 = nn.Conv2d(1, 32, 6)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 2)\n",
    "        self.fc1 = nn.Linear(self.size_fc1, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T10:17:32.032637694Z",
     "start_time": "2023-09-10T10:17:31.656047386Z"
    }
   },
   "id": "559e08652119ef46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code for generating the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36cf683558b80337"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GermanCharacterRecognitionDS(Dataset):\n",
    "    def __init__(self, path_csv, one_hot_encoder, transform=None, target_transform=None, classes=[],\n",
    "                 num_channels=1):\n",
    "        self.path_csv = path_csv\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_lines = self.read_lines_csv(classes)\n",
    "        self.n = len(self.data_lines)\n",
    "        self.classes = classes\n",
    "        self.onehot_encoder = one_hot_encoder\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, image = self.parse_one_line(idx)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        label = self.onehot_encoder.transform(np.array(label).reshape(-1, 1))[0]\n",
    "        return image, label\n",
    "\n",
    "    def read_lines_csv(self, classes):\n",
    "        training_data_file = open(self.path_csv, 'r', encoding=\"latin-1\")\n",
    "        data_lines = training_data_file.readlines()\n",
    "        training_data_file.close()\n",
    "        data_lines = [line for line in data_lines if line[0] in classes]\n",
    "        return data_lines\n",
    "\n",
    "    def parse_one_line(self, index):\n",
    "        line = self.data_lines[index].split(',')\n",
    "        image_np = np.asarray(line[1:1601], dtype=np.float32)\n",
    "        image_np = image_np.reshape(40, 40, 1)\n",
    "        if self.num_channels != 1:\n",
    "            image_np = np.repeat(image_np, self.num_channels, axis=2)\n",
    "        return line[0], image_np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T10:17:32.032924719Z",
     "start_time": "2023-09-10T10:17:31.699808734Z"
    }
   },
   "id": "ad2f938ed134dcd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can define the train loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "901d1b686273cb0a"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "def train_model(data_loaders, model, loss_func, optimizer, device):\n",
    "    print(\"training started\")\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    information = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "    early_stopper = EarlyStopper(patience=hparams[\"early_stopping_patience\"],\n",
    "                             min_delta=hparams[\"early_stopping_threshold\"],\n",
    "                             model_weights=copy.deepcopy(model.state_dict()))\n",
    "    strop_training = False\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        if strop_training == True:\n",
    "            break\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()  \n",
    "            information.reset_metrics()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                print(\"training...\")\n",
    "            else:\n",
    "                print(\"validating...\")                \n",
    "            data_loader = tqdm(data_loaders[phase])\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                information.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "            result_dict = information.calculate_metrics(phase)\n",
    "            # prints the all metrics of the training and validation phase\n",
    "            print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if early_stopper.early_stop(result_dict[\"mcc\"], copy.deepcopy(model.state_dict())):\n",
    "                    print('early stopping')\n",
    "                    strop_training = True\n",
    "    # load best model\n",
    "    model.load_state_dict(early_stopper.best_model_weights) \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T10:17:32.033917637Z",
     "start_time": "2023-09-10T10:17:31.747593814Z"
    }
   },
   "id": "b25f6fa6aa8aa886"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For loading the data we need some helper methods. As stated in the description of the dataset, the representation of each class of the train data set is the same for each class. We also want to make sure that the validation data has the same distribution as the test data, so we need a function which takes a certain amount of samples from each class of the train data set and puts them into the validation data set. To optimize the run time, we save the indices of the train and validation data set in a numpy array. This approach eliminates the need to regenerate the data split each time, thereby significantly reducing processing time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b8014933238ebfd"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_train_and_val_loader(train_data_set, num_samples_validation_data=250):\n",
    "    # Define the ratio for train and validation data\n",
    "    print(\"Splitting train- and val-data ...\")\n",
    "    val_count = dict(zip(classes, len(classes) * [0]))\n",
    "    val_indices = []\n",
    "    train_indices = []\n",
    "    for i in range(len(train_data_set)):\n",
    "        _, label = train_data_set[i]\n",
    "        string_label_list = onehot_encoder.inverse_transform(np.reshape(label, (1, -1)))\n",
    "        label_string = str(string_label_list[0][0])\n",
    "        number = val_count[label_string]\n",
    "        if val_count[label_string] < num_samples_validation_data:\n",
    "            val_count[label_string] += 1\n",
    "            val_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "\n",
    "    np.save(\"val_indices.npy\", np.asarray(val_indices))\n",
    "    np.save(\"train_indices.npy\", np.asarray(train_indices))\n",
    "    train_loader, val_loader = split_train_loader(train_data_set, train_indices, val_indices)\n",
    "    print(\"Splitting done\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def split_train_loader(train_data_set, train_indices, val_indices):\n",
    "    train_loader = torch.utils.data.DataLoader(Subset(train_data_set, train_indices), batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=num_workers)\n",
    "    val_loader = torch.utils.data.DataLoader(Subset(train_data_set, val_indices), batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_class_counts_of_data_loader(data_loader):\n",
    "    labels_count_dict = dict(zip(classes, len(classes) * [0]))\n",
    "    for _, labels in data_loader:\n",
    "        string_label = onehot_encoder.inverse_transform(labels)\n",
    "        for label in string_label:\n",
    "            labels_count_dict[label[0]] += 1\n",
    "    return labels_count_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T10:17:32.049224357Z",
     "start_time": "2023-09-10T10:17:31.748066195Z"
    }
   },
   "id": "788fc58fae7cf1c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can proceed to construct all the necessary data loaders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d57e79ee28e42a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train- and val-data ...\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(35.37502147246886, 75.87412766890324)])\n",
    "train_set = GermanCharacterRecognitionDS(path_train_csv, transform=transform, classes=classes,\n",
    "                                         one_hot_encoder=onehot_encoder, num_channels=1)\n",
    "test_set = GermanCharacterRecognitionDS(path_test_csv, transform=transform, classes=classes,\n",
    "                                        one_hot_encoder=onehot_encoder, num_channels=1)\n",
    "\n",
    "train_loader, val_loader = get_train_and_val_loader(train_set, num_val_samples_per_class)\n",
    "# TODO uncomment this line if you want to use the precalculated indices which speeds up the run time\n",
    "#train_loader, val_loader = split_train_loader(train_set, np.load(\"train_indices.npy\"), np.load(\"val_indices.npy\"))\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(\"train_loader: \" + str(get_class_counts_of_data_loader(train_loader)))\n",
    "print(\"val_loader: \" + str(get_class_counts_of_data_loader(val_loader)))\n",
    "print(\"test_loader: \" + str(get_class_counts_of_data_loader(test_loader)))\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": len(train_loader.dataset), \"val\": len(val_loader.dataset), \"test\": len(test_loader.dataset)}"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-10T10:17:31.791953145Z"
    }
   },
   "id": "d702529aa63cfcce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can start the training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d450ec0b18b1eaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = PyTorchClassifier(len(classes))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "model = train_model(data_loaders, model, loss_func, optimizer, device)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5f7db439103b25e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the training we evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17441b37abf77f04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "information_test = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "model.eval()\n",
    "for inputs, labels in data_loaders[\"test\"]:\n",
    "    inputs = inputs.to(device, non_blocking=True)\n",
    "    labels = labels.to(device, non_blocking=True)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "    information_test.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "result_dict = information_test.calculate_metrics(\"test\")\n",
    "print(\"Test metrics:\")\n",
    "print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f7ec08f16abc393a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5cdd98228fd8a345"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
