{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "First we define some global variables which are used for the whole training process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53a83349ffb7c61d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7ff71c0d5210>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Change the paths accordingly\n",
    "path_train_csv = \"train.csv\"\n",
    "path_test_csv = \"test.csv\"\n",
    "# First we have to select the classes on which we would like to train on\n",
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "dict_classes_to_numbers = dict(zip(classes, range(len(classes))))\n",
    "dict_numbers_to_classes = dict(zip(range(len(classes)), classes))\n",
    "\n",
    "# All available classes\n",
    "#classes = ['!','$','&','(',')','+','0','1','2','3','4','5','6','7','8','9','<','>','?','A','B','C','D','E','F','G','H',\n",
    "#           'I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i',\n",
    "#           'j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','~','ß','α','β','π','φ','€','∑','√','∞',\n",
    "#           '∫']\n",
    "num_classes = len(classes)\n",
    "num_val_samples_per_class = 250\n",
    "# Standard DL-parameters\n",
    "batch_size_train = 64\n",
    "batch_size_val = 256\n",
    "num_workers = 12\n",
    "hparams = {\"num_epochs\": 100, \"early_stopping_patience\": 3, \"early_stopping_threshold\": 0.001}\n",
    "# For getting reproducible results\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:16.101548971Z",
     "start_time": "2023-09-10T17:31:15.646291645Z"
    }
   },
   "id": "f87a7cd9d506d565"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define some helper functions for the training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6071488dbb00355"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0.001, model_weights=None):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_validation_acc = 0\n",
    "        self.best_model_weights = model_weights\n",
    "\n",
    "    def early_stop(self, validation_acc, model_weights):\n",
    "       # If the model improved we store the best weights\n",
    "        if validation_acc > self.best_validation_acc:\n",
    "            self.best_model_weights = model_weights\n",
    "        \n",
    "        # If the model improved more than the threshold  \n",
    "        if validation_acc > self.best_validation_acc + self.min_delta:\n",
    "            self.counter = 0\n",
    "            self.best_validation_acc = validation_acc\n",
    "        else:          \n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            self.counter += 1    \n",
    "        return False\n",
    "\n",
    "\n",
    "class EpochInformation:\n",
    "    def __init__(self, model, device, num_classes, dataset_sizes):\n",
    "        self.mcc_metric = torchmetrics.MatthewsCorrCoef(task='multiclass', num_classes=num_classes).to(device)\n",
    "        self.auc_metric = torchmetrics.AUROC(task='multiclass', num_classes=num_classes).to(device)\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "        self.running_loss = 0.0\n",
    "        self.running_outputs = None\n",
    "        self.running_labels = None\n",
    "        self.model = model\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        self.running_loss = 0.0\n",
    "        self.running_outputs = None\n",
    "        self.running_labels = None            \n",
    "        \n",
    "    def update_metrics_for_batch(self, outputs, loss, inputs, labels):\n",
    "        if self.running_outputs is None:\n",
    "            self.running_outputs = outputs\n",
    "        else:\n",
    "            self.running_outputs = torch.cat((self.running_outputs, outputs), dim=0)\n",
    "        \n",
    "        if self.running_labels is None:\n",
    "            self.running_labels = labels\n",
    "        else:\n",
    "            self.running_labels = torch.cat((self.running_labels, labels), dim=0)\n",
    "\n",
    "        #update the loss\n",
    "        self.running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    def calculate_metrics(self, phase):\n",
    "        loss = self.running_loss / self.dataset_sizes[phase]\n",
    "                \n",
    "        _, predictions = torch.max(self.running_outputs, 1)\n",
    "        comparison = predictions == self.running_labels\n",
    "        corrects = torch.sum(comparison)\n",
    "        \n",
    "        acc = corrects.double() / self.dataset_sizes[phase]\n",
    "        mcc = self.mcc_metric(predictions, self.running_labels)\n",
    "        auc = self.auc_metric(self.running_outputs, self.running_labels)\n",
    "        \n",
    "        # The gradient norm can only be calculated during training\n",
    "        # Also we calculate the weight-norm only once in each training epoch\n",
    "        if self.model.training:\n",
    "            grads = [param.grad.detach().flatten() for param in self.model.parameters() if param.grad is not None]\n",
    "            l2_norm_grads = torch.linalg.vector_norm(torch.cat(grads))\n",
    "            weights = [param.detach().flatten() for param in self.model.parameters()]\n",
    "            l2_norm_weights = torch.linalg.vector_norm(torch.cat(weights)) \n",
    "           \n",
    "        result_dict = {\n",
    "            \"loss\" : loss,\n",
    "            \"acc\"  : acc.item(),\n",
    "            \"mcc\"  : mcc.item(),\n",
    "            \"auc\"  : auc.item()\n",
    "        }\n",
    "        if self.model.training:\n",
    "            result_dict[\"l2_grad\"] = l2_norm_grads.item()\n",
    "            result_dict[\"l2_weights\"] = l2_norm_weights.item()\n",
    "            \n",
    "        return result_dict "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:16.105346673Z",
     "start_time": "2023-09-10T17:31:15.703641323Z"
    }
   },
   "id": "5af3981cde2e146c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define a simple CNN-model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28ec067a47cc96dd"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PyTorchClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PyTorchClassifier, self).__init__()\n",
    "        self.size_fc1 = 256\n",
    "        self.conv1 = nn.Conv2d(1, 32, 6)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 2)\n",
    "        self.fc1 = nn.Linear(self.size_fc1, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:16.115582069Z",
     "start_time": "2023-09-10T17:31:15.757251513Z"
    }
   },
   "id": "559e08652119ef46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code for generating the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36cf683558b80337"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class GermanCharacterRecognitionDS(Dataset):\n",
    "    def __init__(self, path_csv, dict_classes_to_numbers, transform=None, target_transform=None, classes=[],\n",
    "                 num_channels=1):\n",
    "        self.path_csv = path_csv\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_lines = self.read_lines_csv(classes)\n",
    "        self.n = len(self.data_lines)\n",
    "        self.classes = classes\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, image = self.parse_one_line(idx)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)            \n",
    "        # We have to convert the label to an integer value\n",
    "        label = dict_classes_to_numbers[label]\n",
    "        return image, label\n",
    "\n",
    "    def read_lines_csv(self, classes):\n",
    "        training_data_file = open(self.path_csv, 'r', encoding=\"latin-1\")\n",
    "        data_lines = training_data_file.readlines()\n",
    "        training_data_file.close()\n",
    "        data_lines = [line for line in data_lines if line[0] in classes]\n",
    "        return data_lines\n",
    "\n",
    "    def parse_one_line(self, index):\n",
    "        line = self.data_lines[index].split(',')\n",
    "        image_np = np.asarray(line[1:1601], dtype=np.float32)\n",
    "        image_np = image_np.reshape(40, 40, 1)\n",
    "        if self.num_channels != 1:\n",
    "            image_np = np.repeat(image_np, self.num_channels, axis=2)\n",
    "        return line[0], image_np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:16.118002364Z",
     "start_time": "2023-09-10T17:31:15.800810171Z"
    }
   },
   "id": "ad2f938ed134dcd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can define the train loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "901d1b686273cb0a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "def train_model(data_loaders, model, loss_func, optimizer, device):\n",
    "    print(\"training started\")\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    information = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "    early_stopper = EarlyStopper(patience=hparams[\"early_stopping_patience\"],\n",
    "                             min_delta=hparams[\"early_stopping_threshold\"],\n",
    "                             model_weights=copy.deepcopy(model.state_dict()))\n",
    "    strop_training = False\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        if strop_training == True:\n",
    "            break\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()  \n",
    "            information.reset_metrics()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                print(\"training...\")\n",
    "            else:\n",
    "                print(\"validating...\")                \n",
    "            data_loader = tqdm(data_loaders[phase])\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_func(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                information.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "            result_dict = information.calculate_metrics(phase)\n",
    "            # prints the all metrics of the training and validation phase\n",
    "            print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if early_stopper.early_stop(result_dict[\"mcc\"], copy.deepcopy(model.state_dict())):\n",
    "                    print('early stopping')\n",
    "                    strop_training = True\n",
    "    # load best model\n",
    "    model.load_state_dict(early_stopper.best_model_weights) \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:16.118300787Z",
     "start_time": "2023-09-10T17:31:15.801461252Z"
    }
   },
   "id": "b25f6fa6aa8aa886"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For loading the data we need some helper methods. As stated in the description of the dataset, the representation of each class of the train data set is the same for each class. We also want to make sure that the validation data has the same distribution as the test data, so we need a function which takes a certain amount of samples from each class of the train data set and puts them into the validation data set. To optimize the run time, we save the indices of the train and validation data set in a numpy array. This approach eliminates the need to regenerate the data split each time, thereby significantly reducing processing time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b8014933238ebfd"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_train_and_val_loader(train_data_set, num_samples_validation_data=250):\n",
    "    # Define the ratio for train and validation data\n",
    "    print(\"Splitting train- and val-data ...\")\n",
    "    val_count = dict(zip(classes, len(classes) * [0]))\n",
    "    val_indices = []\n",
    "    train_indices = []\n",
    "    for i in range(len(train_data_set)):\n",
    "        _, label = train_data_set[i]\n",
    "        label_string = dict_numbers_to_classes[label[0]]\n",
    "        if val_count[label_string] < num_samples_validation_data:\n",
    "            val_count[label_string] += 1\n",
    "            val_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "\n",
    "    np.save(\"val_indices.npy\", np.asarray(val_indices))\n",
    "    np.save(\"train_indices.npy\", np.asarray(train_indices))\n",
    "    train_set, val_set = split_train_set(train_data_set, train_indices, val_indices)\n",
    "    print(\"Splitting done\")\n",
    "    return train_set, val_set\n",
    "\n",
    "def split_train_set(train_data_set, train_indices, val_indices):\n",
    "    train_set = Subset(train_data_set, train_indices)\n",
    "    val_set = Subset(train_data_set, val_indices)\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_class_counts_of_data_loader(data_loader):\n",
    "    labels_count_dict = dict(zip(classes, len(classes) * [0]))\n",
    "    for _, labels in data_loader:\n",
    "        string_labels = [dict_numbers_to_classes[number] for number in labels.tolist()]\n",
    "        for label in string_labels:\n",
    "            labels_count_dict[label] += 1\n",
    "    return labels_count_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:16.135314775Z",
     "start_time": "2023-09-10T17:31:15.851870315Z"
    }
   },
   "id": "788fc58fae7cf1c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can proceed to construct all the necessary data loaders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d57e79ee28e42a4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader: {'0': 4541, '1': 4203, '2': 4168, '3': 4120, '4': 4019, '5': 3966, '6': 4235, '7': 4161, '8': 4213, '9': 4125}\n",
      "val_loader: {'0': 250, '1': 250, '2': 250, '3': 250, '4': 250, '5': 250, '6': 250, '7': 250, '8': 250, '9': 250}\n",
      "test_loader: {'0': 500, '1': 500, '2': 500, '3': 500, '4': 500, '5': 500, '6': 500, '7': 500, '8': 500, '9': 500}\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "# We normalize with the men and std of the train set\n",
    "standard_transforms = [transforms.ToTensor(),transforms.Normalize(35.37502147246886, 75.87412766890324)]\n",
    "test_set = GermanCharacterRecognitionDS(path_test_csv, transform=transforms.Compose(standard_transforms), classes=classes,\n",
    "                                        dict_classes_to_numbers=dict_classes_to_numbers, num_channels=1)\n",
    "train_set = GermanCharacterRecognitionDS(path_train_csv, transform=None, classes=classes,\n",
    "                                         dict_classes_to_numbers=dict_classes_to_numbers, num_channels=1)\n",
    "# TODO comment the following line after the first run\n",
    "#train_loader, val_loader = get_train_and_val_loader(train_set, num_val_samples_per_class)\n",
    "# TODO uncomment this line if you want to use the precalculated indices which speeds up the run time\n",
    "train_set, val_set = split_train_set(train_set, np.load(\"train_indices.npy\"), np.load(\"val_indices.npy\"))\n",
    "\n",
    "train_transforms = standard_transforms + [transforms.RandomRotation(30), transforms.RandomGrayscale(p=0.1), \n",
    "                                          transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))]\n",
    "train_set.dataset.transform = transforms.Compose(train_transforms)\n",
    "val_set.dataset.transform = transforms.Compose(standard_transforms)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True, num_workers=num_workers,                                                   generator=g)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size_val, shuffle=False, num_workers=num_workers,\n",
    "                                         generator=g)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_val, shuffle=False, num_workers=num_workers,\n",
    "                                          generator=g)\n",
    "\n",
    "class_counts_train = get_class_counts_of_data_loader(train_loader)\n",
    "class_counts_val = get_class_counts_of_data_loader(val_loader)\n",
    "class_counts_test = get_class_counts_of_data_loader(test_loader)\n",
    "\n",
    "print(\"train_loader: \" + str(class_counts_train))\n",
    "print(\"val_loader: \" + str(class_counts_val))\n",
    "print(\"test_loader: \" + str(class_counts_test))\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": len(train_loader.dataset), \"val\": len(val_loader.dataset), \"test\": len(test_loader.dataset)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:23.282688120Z",
     "start_time": "2023-09-10T17:31:15.852529090Z"
    }
   },
   "id": "2530143e3e55b010"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also calculate the class weights in order to use a weighted loss function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2a555fe597dd9fe"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights:  {'0': 0.09183375537395477, '1': 0.09921891242265701, '2': 0.10005208849906921, '3': 0.10121773928403854, '4': 0.10376140475273132, '5': 0.10514803230762482, '6': 0.09846921265125275, '7': 0.10022040456533432, '8': 0.09898340702056885, '9': 0.101095050573349}\n"
     ]
    }
   ],
   "source": [
    "class_weights = []\n",
    "number_train_values = len(train_loader.dataset)\n",
    "for class_label in classes:\n",
    "    weight = float(number_train_values) / class_counts_train[class_label]\n",
    "    class_weights.append(weight)\n",
    "class_weights = torch.tensor(class_weights)\n",
    "sum_class_weights = torch.sum(class_weights)\n",
    "class_weights = class_weights / sum_class_weights\n",
    "print(\"class weights: \", str(dict(zip(classes, class_weights.tolist()))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:31:23.342581780Z",
     "start_time": "2023-09-10T17:31:23.285035630Z"
    }
   },
   "id": "d702529aa63cfcce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can start the training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d450ec0b18b1eaa"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "training started\n",
      "Epoch 0/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.3019 acc: 0.0992 mcc: -0.0059 auc: 0.5584\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:07<00:00, 86.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2868 acc: 0.9073 mcc: 0.897 auc: 0.9948 l2_grad: 2.8797 l2_weights: 20.2377\n",
      "Epoch 1/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1916 acc: 0.9484 mcc: 0.9432 auc: 0.9988\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:07<00:00, 84.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.087 acc: 0.9735 mcc: 0.9706 auc: 0.9994 l2_grad: 2.4594 l2_weights: 22.4498\n",
      "Epoch 2/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.074 acc: 0.978 mcc: 0.9756 auc: 0.9995\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:08<00:00, 80.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0636 acc: 0.9809 mcc: 0.9788 auc: 0.9996 l2_grad: 0.0334 l2_weights: 24.457\n",
      "Epoch 3/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0763 acc: 0.9784 mcc: 0.976 auc: 0.9996\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:07<00:00, 82.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0524 acc: 0.9841 mcc: 0.9824 auc: 0.9997 l2_grad: 0.3741 l2_weights: 26.4261\n",
      "Epoch 4/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0743 acc: 0.9764 mcc: 0.9738 auc: 0.9995\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:08<00:00, 78.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0425 acc: 0.9869 mcc: 0.9855 auc: 0.9998 l2_grad: 0.3011 l2_weights: 28.3459\n",
      "Epoch 5/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0964 acc: 0.9732 mcc: 0.9703 auc: 0.9994\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:08<00:00, 80.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0385 acc: 0.9877 mcc: 0.9863 auc: 0.9999 l2_grad: 0.0028 l2_weights: 30.3312\n",
      "Epoch 6/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.067 acc: 0.9836 mcc: 0.9818 auc: 0.9996\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:09<00:00, 66.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.033 acc: 0.9903 mcc: 0.9892 auc: 0.9999 l2_grad: 0.003 l2_weights: 32.172\n",
      "Epoch 7/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0984 acc: 0.9804 mcc: 0.9783 auc: 0.9993\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:10<00:00, 64.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0307 acc: 0.9905 mcc: 0.9895 auc: 0.9999 l2_grad: 0.0177 l2_weights: 34.1778\n",
      "Epoch 8/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0718 acc: 0.9828 mcc: 0.9809 auc: 0.9996\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:09<00:00, 65.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0264 acc: 0.9916 mcc: 0.9906 auc: 0.9999 l2_grad: 0.0141 l2_weights: 36.0953\n",
      "Epoch 9/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0743 acc: 0.9824 mcc: 0.9805 auc: 0.9994\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:09<00:00, 68.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0239 acc: 0.9928 mcc: 0.992 auc: 0.9999 l2_grad: 0.1738 l2_weights: 37.7499\n",
      "Epoch 10/99\n",
      "----------\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0836 acc: 0.984 mcc: 0.9822 auc: 0.9995\n",
      "early stopping\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:09<00:00, 68.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0208 acc: 0.9933 mcc: 0.9925 auc: 1.0 l2_grad: 0.0023 l2_weights: 39.5625\n",
      "Epoch 11/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = PyTorchClassifier(len(classes))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Put the tensors on the GPU\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001, weight_decay=0.000)\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "model = train_model(data_loaders, model, loss_func, optimizer, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:33:14.174185077Z",
     "start_time": "2023-09-10T17:31:23.327438241Z"
    }
   },
   "id": "5f7db439103b25e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the training we evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17441b37abf77f04"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics:\n",
      "loss: 0.092 acc: 0.9786 mcc: 0.9762 auc: 0.9994\n"
     ]
    }
   ],
   "source": [
    "information_test = EpochInformation(model, device, num_classes, dataset_sizes)\n",
    "model.eval()\n",
    "for inputs, labels in data_loaders[\"test\"]:\n",
    "    inputs = inputs.to(device, non_blocking=True)\n",
    "    labels = labels.to(device, non_blocking=True)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "    information_test.update_metrics_for_batch(outputs, loss, inputs, labels)\n",
    "\n",
    "result_dict = information_test.calculate_metrics(\"test\")\n",
    "print(\"Test metrics:\")\n",
    "print(\" \".join(name + \": \" + str(round(value, 4)) for name, value in result_dict.items()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:33:15.956432313Z",
     "start_time": "2023-09-10T17:33:14.177929430Z"
    }
   },
   "id": "f7ec08f16abc393a"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T17:33:15.968358980Z",
     "start_time": "2023-09-10T17:33:15.958817676Z"
    }
   },
   "id": "5cdd98228fd8a345"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
